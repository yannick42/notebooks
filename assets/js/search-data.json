{
  
    
        "post0": {
            "title": "[DRAFT] Titanic dataset",
            "content": ". Setup . %%capture !pip install kaggle --upgrade . %%capture import os from getpass import getpass kaggle_username = input(&quot;Kaggle USERNAME : &quot;) os.environ[&#39;KAGGLE_USERNAME&#39;] = kaggle_username kaggle_key = getpass(&quot;Kaggle KEY for &quot;+os.environ.get(&#39;KAGGLE_USERNAME&#39;)+&quot;: &quot;) os.environ[&quot;KAGGLE_KEY&quot;] = kaggle_key . Kaggle USERNAME : yannick42 Kaggle KEY for yannick42: ·········· . !kaggle competitions download -c titanic -p my_data !cd my_data &amp;&amp; unzip titanic.zip . Downloading titanic.zip to my_data 0% 0.00/34.1k [00:00&lt;?, ?B/s] 100% 34.1k/34.1k [00:00&lt;00:00, 17.4MB/s] Archive: titanic.zip inflating: gender_submission.csv inflating: test.csv inflating: train.csv . # my_data gender_submission.csv # my_data test.csv # my_data train.csv !ls -la my_data . total 136 drwxr-xr-x 2 root root 4096 Apr 26 17:59 . drwxr-xr-x 1 root root 4096 Apr 26 17:59 .. -rw-r--r-- 1 root root 3258 Dec 11 2019 gender_submission.csv -rw-r--r-- 1 root root 28629 Dec 11 2019 test.csv -rw-r--r-- 1 root root 34877 Apr 26 17:59 titanic.zip -rw-r--r-- 1 root root 61194 Dec 11 2019 train.csv . Load data + transformations . import pandas as pd # Pandas basics : https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html def feature_eng(df): # Gender: str -&gt; int gender = {&#39;male&#39;: 1, &#39;female&#39;: 2} df[&#39;Sex&#39;] = [gender[item] for item in df[&#39;Sex&#39;]] # Fill missing values with mean df[&#39;Age&#39;].fillna((df[&#39;Age&#39;].mean()), inplace=True) df[&#39;Fare&#39;].fillna((df[&#39;Fare&#39;].mean()), inplace=True) # see recommendations by https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/ df[&#39;Age*Class&#39;] = df[&#39;Age&#39;] * df[&#39;Pclass&#39;] df[&#39;SibSp&#39;].fillna(0, inplace=True) # Number of Siblings/Spouses Aboard df[&#39;Parch&#39;].fillna(0, inplace=True) # Number of Parents/Children Aboard df[&#39;Family_Size&#39;] = df[&#39;SibSp&#39;] + df[&#39;Parch&#39;] df[&#39;Fare_Per_Person&#39;] = df[&#39;Fare&#39;]/(df[&#39;Family_Size&#39;]+1) df.Embarked.fillna(0, inplace=True) # unknown embarkation place embarked = {&#39;S&#39;: 1, &#39;C&#39;: 2, &#39;Q&#39;: 3, 0: 0} # Southampton (UK, Apr 10th), Cherbourg (France, Apr 10th), Queenstown (&quot;Ireland&quot;..., Apr 11th) df[&#39;Embarked&#39;] = [embarked[item] for item in df[&#39;Embarked&#39;]] return df ### ### TRAIN DATA (+Transformations) ### df = pd.read_csv(&#39;my_data/train.csv&#39;) df = feature_eng(df) #df.describe() ### ### TEST DATA (+Transformations) ### df_test = pd.read_csv(&#39;my_data/test.csv&#39;) df_test = feature_eng(df_test) #df_test.describe() . General informations . The Titanic departed from Southampton on April 10th, to Cherbourg, to Queenstown (the 11th). She sinked the 15th of April 1912 . # the 13 youngest deads below 5 y.o. :( df[(df.Survived == 0) &amp; (df.Age &lt;= 5)].sort_values(by=&#39;Age&#39;) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Age*Class Family_Size Fare_Per_Person . 164 165 | 0 | 3 | Panula, Master. Eino Viljami | 1 | 1.0 | 4 | 1 | 3101295 | 39.6875 | NaN | 1 | 3.0 | 5 | 6.614583 | . 386 387 | 0 | 3 | Goodwin, Master. Sidney Leonard | 1 | 1.0 | 5 | 2 | CA 2144 | 46.9000 | NaN | 1 | 3.0 | 7 | 5.862500 | . 7 8 | 0 | 3 | Palsson, Master. Gosta Leonard | 1 | 2.0 | 3 | 1 | 349909 | 21.0750 | NaN | 1 | 6.0 | 4 | 4.215000 | . 16 17 | 0 | 3 | Rice, Master. Eugene | 1 | 2.0 | 4 | 1 | 382652 | 29.1250 | NaN | 3 | 6.0 | 5 | 4.854167 | . 119 120 | 0 | 3 | Andersson, Miss. Ellis Anna Maria | 2 | 2.0 | 4 | 2 | 347082 | 31.2750 | NaN | 1 | 6.0 | 6 | 4.467857 | . 205 206 | 0 | 3 | Strom, Miss. Telma Matilda | 2 | 2.0 | 0 | 1 | 347054 | 10.4625 | G6 | 1 | 6.0 | 1 | 5.231250 | . 297 298 | 0 | 1 | Allison, Miss. Helen Loraine | 2 | 2.0 | 1 | 2 | 113781 | 151.5500 | C22 C26 | 1 | 2.0 | 3 | 37.887500 | . 642 643 | 0 | 3 | Skoog, Miss. Margit Elizabeth | 2 | 2.0 | 3 | 2 | 347088 | 27.9000 | NaN | 1 | 6.0 | 5 | 4.650000 | . 824 825 | 0 | 3 | Panula, Master. Urho Abraham | 1 | 2.0 | 4 | 1 | 3101295 | 39.6875 | NaN | 1 | 6.0 | 5 | 6.614583 | . 374 375 | 0 | 3 | Palsson, Miss. Stina Viola | 2 | 3.0 | 3 | 1 | 349909 | 21.0750 | NaN | 1 | 9.0 | 4 | 4.215000 | . 63 64 | 0 | 3 | Skoog, Master. Harald | 1 | 4.0 | 3 | 2 | 347088 | 27.9000 | NaN | 1 | 12.0 | 5 | 4.650000 | . 171 172 | 0 | 3 | Rice, Master. Arthur | 1 | 4.0 | 4 | 1 | 382652 | 29.1250 | NaN | 3 | 12.0 | 5 | 4.854167 | . 850 851 | 0 | 3 | Andersson, Master. Sigvard Harald Elias | 1 | 4.0 | 4 | 2 | 347082 | 31.2750 | NaN | 1 | 12.0 | 6 | 4.467857 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Plots . Pie Charts . import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap row = 2 col = 3 fig, ax = plt.subplots(row, col, figsize=(20,8)) list_columns = [&#39;Pclass&#39;, &#39;Survived&#39;, &#39;Embarked&#39;, &#39;Family_Size&#39;, &#39;Sex&#39;, None] # to generate a gradient of colors colors = [(.5, 0, 0), (1, 0, 1)] # from first color to the last cm = LinearSegmentedColormap.from_list(&quot;Custom&quot;, colors, N=11) # 11 shades colors = [ [&#39;yellow&#39;, &#39;orange&#39;, &#39;red&#39;], [&#39;grey&#39;, &#39;lightblue&#39;], [&#39;black&#39;, &#39;green&#39;, &#39;lightgreen&#39;, &#39;cyan&#39;], cm(np.linspace(0, 1, num=11)), # equally &quot;spaced&quot; 11 colors... [&#39;blue&#39;, &#39;pink&#39;], ] i = 0 for column in list_columns: axis = ax[i//col][i%(row+1)] if column is None: axis.axis(&#39;off&#39;) # hide else: s = df.groupby([column]).size().to_frame(&#39;size&#39;).reset_index() axis.set_title(column) # plot pie chart patches, texts = axis.pie(s[&#39;size&#39;], colors=colors[i], startangle=90) # add a legend axis.legend(patches, s[column].unique(), loc=&quot;best&quot;) i=i+1 plt.tight_layout() . . Scatterplot . 4 numerical variables in one scatterplot : x=Age / y=Fare / color=Pclass : Survived or not . from matplotlib.pyplot import figure import seaborn as sns fig, ax = plt.subplots(1, 2, figsize=(16,8)) sns.set(style=&quot;whitegrid&quot;) figure(figsize=(20, 6), dpi=80) markers = {0: &quot;X&quot;, 1: &quot;o&quot;} sns.scatterplot( ax=ax[0], data=df, x=&quot;Age&quot;, y=&quot;Fare&quot;, hue=&quot;Pclass&quot;, palette=[&#39;green&#39;,&#39;blue&#39;,&#39;grey&#39;], style=&quot;Survived&quot;, size=&quot;Survived&quot;, sizes=(15, 65), markers=markers ) # same with log scale s = sns.scatterplot( ax=ax[1], data=df, x=&quot;Age&quot;, y=&quot;Fare&quot;, hue=&quot;Pclass&quot;, palette=[&#39;green&#39;,&#39;blue&#39;,&#39;grey&#39;], style=&quot;Survived&quot;, size=&quot;Survived&quot;, sizes=(15, 65), markers=markers ); s.set_yscale(&quot;log&quot;) s.set_title(&quot;With log scale (Fare)&quot;) ticks = [1, 10, 100, 1000] s.set_yticks(ticks); . . &lt;Figure size 1600x480 with 0 Axes&gt; . fig, ax = plt.subplots(1, 2, figsize=(16,8)) sns.histplot(ax=ax[0], data=df, x=&quot;Age&quot;, kde=True) sns.kdeplot(ax=ax[1], data=df, x=&quot;Age&quot;, y=&quot;Fare&quot;, fill=True); . Ridgeline plot . Age distributions per passenger class . %%capture !pip install joypy . from joypy import joyplot import matplotlib.pyplot as plt from matplotlib import cm joyplot(df, by=&#39;Pclass&#39;, column=&#39;Age&#39;, colormap=cm.autumn, fade = True) plt.xlabel(&quot;Age per class&quot;) plt.show() . . Fisher&#39;s exact tests . Test for independence&#39;s results : Male are dying more often than Female (obvious fact) . from scipy.stats import fisher_exact cross = pd.crosstab(df[&#39;Sex&#39;], df[&#39;Survived&#39;]) print(cross) print(&quot;-&quot;*30) oddsr, p = fisher_exact(table=cross.to_numpy(), alternative=&#39;greater&#39;) print(&quot;odds ratio :&quot;, oddsr) print(&quot;p-value :&quot;, p) . Survived 0 1 Sex 1 468 109 2 81 233 odds ratio : 12.3506625891947 p-value : 3.5925132664684234e-60 . Test for independence&#39;s results : Young people are dying less than adults (obvious fact) . from scipy.stats import fisher_exact cross = pd.crosstab(df[&#39;Age&#39;] &lt; 18, df[&#39;Survived&#39;]) print(cross) print(&quot;-&quot;*30) oddsr, p = fisher_exact(table=cross.to_numpy(), alternative=&#39;greater&#39;) print(&quot;odds ratio :&quot;, oddsr) print(&quot;p-value :&quot;, p) . Survived 0 1 Age False 497 281 True 52 61 odds ratio : 2.0748015329865863 p-value : 0.00023180674713449808 .",
            "url": "https://notebooks.yannick42.dev/eda/ml/2022/04/24/Titanic_dataset.html",
            "relUrl": "/eda/ml/2022/04/24/Titanic_dataset.html",
            "date": " • Apr 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Superstore Sales Dataset (WIP)",
            "content": ". . Warning: This is a Work in Progress On the &quot;Superstore Sales&quot; dataset : https://www.kaggle.com/datasets/rohitsahoo/sales-forecasting . Setup . %%capture !pip install kaggle --upgrade . %%capture import os from getpass import getpass kaggle_username = input(&quot;Kaggle USERNAME : &quot;) os.environ[&#39;KAGGLE_USERNAME&#39;] = kaggle_username kaggle_key = getpass(&quot;Kaggle KEY for &quot;+os.environ.get(&#39;KAGGLE_USERNAME&#39;)+&quot;: &quot;) os.environ[&quot;KAGGLE_KEY&quot;] = kaggle_key . Kaggle USERNAME : yannick42 Kaggle KEY for yannick42: ·········· . !kaggle datasets download -d rohitsahoo/sales-forecasting !unzip -n sales-forecasting.zip !ls -la . Downloading sales-forecasting.zip to /content 0% 0.00/480k [00:00&lt;?, ?B/s] 100% 480k/480k [00:00&lt;00:00, 69.8MB/s] Archive: sales-forecasting.zip inflating: train.csv total 2580 drwxr-xr-x 1 root root 4096 Apr 24 05:49 . drwxr-xr-x 1 root root 4096 Apr 24 05:49 .. drwxr-xr-x 4 root root 4096 Apr 19 14:22 .config -rw-r--r-- 1 root root 491942 Apr 24 05:49 sales-forecasting.zip drwxr-xr-x 1 root root 4096 Apr 19 14:23 sample_data -rw-r--r-- 1 root root 2129689 Sep 11 2020 train.csv . Load &amp; Display some data . %matplotlib inline import pandas as pd import numpy as np np.random.seed(42) from matplotlib import pyplot as plt # for visualizations (scatter plot, boxplot, ...) import seaborn as sns &quot;&quot;&quot; 18 columns : - 1 numeric - sales (in $) - 14 strings - order date - ship date - category - sub-category - ... &quot;&quot;&quot; df = pd.read_csv(&#39;train.csv&#39;, parse_dates=True, infer_datetime_format=True) . &quot;&quot;&quot; - TODO : Postal Code could be filled by using the city name and using what other lines with the same city name uses ... ? (-&gt; only 11 rows have this problem on 9800 sales) &quot;&quot;&quot; df[&#39;Sales&#39;].describe() . count 9800.000000 mean 230.769059 std 626.651875 min 0.444000 25% 17.248000 50% 54.490000 75% 210.605000 max 22638.480000 Name: Sales, dtype: float64 . df.head(3) # by default, the first 5 rows . level_0 index Order Date 2 Row ID Order ID Order Date Ship Date Ship Mode Customer ID Customer Name ... Region Product ID Category Sub-Category Product Name Sales month year Duration DoW . 0 0 | 0 | 2017-11-08 | 1 | CA-2017-152156 | 08/11/2017 | 11/11/2017 | Second Class | CG-12520 | Claire Gute | ... | South | FUR-BO-10001798 | Furniture | Bookcases | Bush Somerset Collection Bookcase | 261.96 | 8 | 2017 | 3.0 | 2 | . 1 1 | 1 | 2017-11-08 | 2 | CA-2017-152156 | 08/11/2017 | 11/11/2017 | Second Class | CG-12520 | Claire Gute | ... | South | FUR-CH-10000454 | Furniture | Chairs | Hon Deluxe Fabric Upholstered Stacking Chairs,... | 731.94 | 8 | 2017 | 3.0 | 2 | . 2 2 | 2 | 2017-06-12 | 3 | CA-2017-138688 | 12/06/2017 | 16/06/2017 | Second Class | DV-13045 | Darrin Van Huff | ... | West | OFF-LA-10000240 | Office Supplies | Labels | Self-Adhesive Address Labels for Typewriters b... | 14.62 | 12 | 2017 | 4.0 | 0 | . 3 rows × 25 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Various info : . 4922 orders with 14 items at most by order, from 529 cities by 793 customers (??) | There is 17 sub-categories in 3 category | The 5 cities with the most unique customers are : New York City 349 Los Angeles 300 Philadelphia 237 San Francisco 230 Seattle 187 . | The number of unique customers per region West 681 East 669 Central 626 South 509 . | . print(df[[&#39;Order ID&#39;]].value_counts().head()) print() # number of unique cities print(df[[&#39;City&#39;]].nunique()) print() # number of unique customer (= count distinct in SQL...) print(df[[&#39;Customer ID&#39;]].nunique()) print() # Cities print(df[[&#39;City&#39;, &#39;Customer ID&#39;]].groupby([&#39;City&#39;]).nunique().sort_values(by=&#39;Customer ID&#39;, ascending=False).head()) print() # Region print(df[[&#39;Region&#39;, &#39;Customer ID&#39;]].groupby([&#39;Region&#39;]).nunique().sort_values(by=&#39;Customer ID&#39;, ascending=False)) print() # Category &amp; Sub-caterogy print(df[[&#39;Category&#39;]].nunique()) print(df[[&#39;Sub-Category&#39;]].nunique()) . Order ID CA-2018-100111 14 CA-2018-157987 12 US-2017-108504 11 CA-2017-165330 11 CA-2016-131338 10 dtype: int64 City 529 dtype: int64 Customer ID 793 dtype: int64 Customer ID City New York City 349 Los Angeles 300 Philadelphia 237 San Francisco 230 Seattle 187 Customer ID Region West 681 East 669 Central 626 South 509 Category 3 dtype: int64 Sub-Category 17 dtype: int64 . Best sellers . Best selling top-5 products . best_selling = df.groupby(&#39;Product ID&#39;).sum([&#39;Sales&#39;, &#39;Product ID&#39;]).sort_values(by=&#39;Sales&#39;, ascending=False).reset_index()[[&#39;Product ID&#39;, &#39;Sales&#39;]].head(5) print(best_selling) . Product ID Sales 0 TEC-CO-10004722 61599.824 1 OFF-BI-10003527 27453.384 2 TEC-MA-10002412 22638.480 3 FUR-CH-10002024 21870.576 4 OFF-BI-10001359 19823.479 . Best selling top-5 sub-categories . best_selling_subcat = df.groupby(&#39;Sub-Category&#39;).sum([&#39;Sales&#39;, &#39;Sub-Category&#39;]).sort_values(by=&#39;Sales&#39;, ascending=False).reset_index()[[&#39;Sub-Category&#39;, &#39;Sales&#39;]].head(5) print(best_selling_subcat) . Sub-Category Sales 0 Phones 327782.448 1 Chairs 322822.731 2 Storage 219343.392 3 Tables 202810.628 4 Binders 200028.785 . Various charts . Sales&#39; monthly evolution per sub-category . from pandas.tseries.offsets import MonthEnd # transform &quot;string&quot; to date &quot;number&quot; df[&#39;month&#39;] = pd.to_datetime(df[&#39;Order Date&#39;]).dt.month.astype(int) df[&#39;year&#39;] = pd.to_datetime(df[&#39;Order Date&#39;]).dt.year.astype(int) #.to_period(&#39;Y&#39;) =&gt; ?? #print(best_selling.columns) # Product ID, Sales best_selling_df = df.loc[df[&#39;Sub-Category&#39;].isin(best_selling_subcat[&#39;Sub-Category&#39;])] # 4121 rows (order item) #print(best_selling_df.shape) # (4121, 20) #print(best_selling_df) # format to datetime # SettingWithCopyWarning... #best_selling_df.loc[:, [&quot;Order Date (datetime)&quot;]] = pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;) # set day to first day of month ... test = best_selling_df[[&#39;year&#39;, &#39;month&#39;]] #.assign(day=1) test.loc[:, &#39;day&#39;] = 1 # --&gt; SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. best_selling_df.loc[:, &#39;month_date&#39;] = pd.to_datetime(test[[&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]]) # sum aggregation dfPerSubCategory = best_selling_df.groupby([&#39;Sub-Category&#39;, &#39;month_date&#39;]).sum(&#39;Sales&#39;).reset_index() #print(dfPerSubCategory.columns) # &#39;Sub-Category&#39;, &#39;month_date&#39;, &#39;Row ID&#39;, &#39;Postal Code&#39;, &#39;Sales&#39;, &#39;month&#39; # plot lines per sub-categories plt.figure(figsize=(25,5)) ax = sns.lineplot(data=dfPerSubCategory, x=&#39;month_date&#39;, y=&#39;Sales&#39;, hue=&#39;Sub-Category&#39;) ranges = best_selling_df.groupby(&#39;year&#39;)[&#39;month_date&#39;].agg([&#39;min&#39;, &#39;max&#39;]) colors = { 2015: &#39;red&#39;, 2016: &#39;green&#39;, 2017: &#39;orange&#39;, 2018: &#39;blue&#39; } for i, row in ranges.iterrows(): ax.axvspan(xmin=row[&#39;min&#39;], xmax=row[&#39;max&#39;] + MonthEnd(1), facecolor=colors[i], alpha=0.15) . . /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self.obj[key] = value . Sample distribution . Boxplot . The boxplot shows the sales $ text{in } $$, distribution is skewed. There is a lot of &quot;outliers&quot; (higher amount orders), but the median is $ $54$ . plt.figure(figsize=(25,5)) ax = sns.boxplot( y=&#39;Segment&#39;, x=&#39;Sales&#39;, orient=&quot;h&quot;, # horizontal data=df.query(&#39;Segment == &quot;Consumer&quot; | Segment == &quot;Corporate&quot; | Segment == &quot;Home Office&quot;&#39;), showmeans=True, showfliers=True, # (default=None) False to remove outliers flierprops=dict( marker=&#39;x&#39;, markerfacecolor=None, markersize=6, markeredgecolor=&#39;red&#39; ) ) ax.set(xlabel=&#39;Sales in $&#39;, xlim=(0, None)) plt.show() . . Skewness &amp; Kurtosis . Sales : (highly) positive skew (tail longer toward the right) and leptokurtic (kurtosis) because it is above 3, pointy, it&#39;s an indicator that data has heavy outliers . | Duration : fairly symmetrical (between -0.5 and 0.5, 0=symmetrical) and platykurtic (below 3), which is also negative so less pointy (slightly flatter) than the normal distribution . | . print(&quot;Sales :&quot;) # Skewness print(&quot;skewness : &quot;, df[&#39;Sales&#39;].skew()) # Kurtosis print(&quot;kurtosis : &quot;, df[&#39;Sales&#39;].kurt()) df[&quot;Duration&quot;] = (pd.to_datetime(df[&quot;Ship Date&quot;], format=&#39;%d/%m/%Y&#39;) - pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;)).astype(&#39;timedelta64[h]&#39;).astype(np.int32) / 24 print(&quot; nShipping duration :&quot;) # Skewness print(&quot;skewness : &quot;, df[&#39;Duration&#39;].skew()) # Kurtosis print(&quot;kurtosis : &quot;, df[&#39;Duration&#39;].kurt()) . Sales : skewness : 12.983482865034619 kurtosis : 304.4450883210413 Shipping duration : skewness : -0.4273575345243775 kurtosis : -0.280888535513812 . IQR &amp; Outliers . Count the &quot;upper&quot; outliers with the IQR $(Q3-Q1)$ : if greater than $1.5 times IQR$ . IQR = df[&quot;Sales&quot;].quantile(0.75) - df[&quot;Sales&quot;].quantile(0.25) outliers_cnt = len(df[df.Sales &gt; (IQR*1.5)]) total_rows = len(df) print(str(outliers_cnt) + &quot; on &quot; + str(total_rows) + &quot; rows =&gt; &quot; + str(round(outliers_cnt/total_rows*100, 2)) + &quot;% of outliers&quot;) . 1921 outliers on 9800 = 19.6% . QQ-Plot : test for normality . aka. probability plot, it shows a strongly nonlinear pattern, suggesting that the data are not distributed as a standard normal . https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot | . import statsmodels.api as sm sm.qqplot(df[&#39;Sales&#39;], line=&#39;q&#39;, fit=True) # q : line fit through the quartiles, fit : ? plt.plot(); . Monthly Sales (heatmaps) . The number of sales seems to be increasing each year . plt.figure(figsize=(14,6)) #print(df.month.head()) #print(df.year.head()) df_grouped = df.groupby([&#39;month&#39;, &#39;year&#39;]).size().to_frame(&#39;size&#39;) # to name the size column... &#39;cause size() returns a Series object total_sales = df_grouped[&#39;size&#39;].sum() #print(df_grouped) piv = pd.pivot_table(df_grouped, values=&quot;size&quot;,index=[&quot;year&quot;], columns=[&quot;month&quot;], fill_value=0) sns.heatmap(piv, xticklabels=[&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;], annot=True, fmt=&quot;d&quot;, linewidths=0.5, cmap=&quot;Blues&quot;); . . September, and the end of the year seems to be the most profitable months of the year . plt.figure(figsize=(14,6)) # generated in the cell above df_grouped_dollar = df.groupby([&#39;month&#39;, &#39;year&#39;]).sum(&#39;Sales&#39;) piv_dollar = pd.pivot_table(df_grouped_dollar, values=&quot;Sales&quot;,index=[&quot;year&quot;], columns=[&quot;month&quot;], fill_value=0) ax = sns.heatmap(piv_dollar, xticklabels=[&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;], annot=True, fmt=&quot;g&quot;, linewidths=1.5, cmap=&quot;Greens&quot;) ax.set_title(&#39;Sales in $&#39;) total_sales_in_dollar = df_grouped_dollar[&#39;Sales&#39;].sum() print(&quot;Total value over 4 years : &quot;+str(round(total_sales_in_dollar / 1000 / 1000, 3))+&quot; M$&quot;) print(&quot;Mean sale value : &quot;+str(round(total_sales_in_dollar/total_sales))+&quot;$&quot;) plt.show() . . Total value over 4 years : 2.262 M$ Mean sale value : 231$ . by Day of Week . Which day of week makes the most sales (in dollars) ? Tuesdays &amp; Saturdays . df[&quot;DoW&quot;] = pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;).dt.dayofweek # bar colors colors = [&quot;#9999FF&quot;, &quot;#8088EE&quot;, &quot;#6677DD&quot;, &quot;#4D66CC&quot;, &quot;#3355BB&quot;, &quot;#1A44AA&quot;, &quot;#003399&quot;] sns.set_palette(sns.color_palette(colors)) sns.barplot(x=[&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;], y=df.groupby([&quot;DoW&quot;]).sum(&#39;Sales&#39;)[&#39;Sales&#39;]); . colors = [&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;orange&quot;] sns.set_palette(sns.color_palette(colors)) # sns.catplot(x=&quot;DoW&quot;, y=&quot;Sales&quot;, hue=&quot;Ship Mode&quot;, kind=&quot;swarm&quot;, data=df.query(&#39;State == &quot;California&quot;&#39;), height=8, aspect=18/8); . /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 49.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 56.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 43.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 31.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 47.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 52.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 48.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) . US Map . A few states represents most of the sales (in $) . import matplotlib.patches as mpatches import matplotlib.pyplot as plt import matplotlib as mpl import matplotlib.patheffects as PathEffects import shapely.geometry as sgeom import cartopy.crs as ccrs import cartopy.io.shapereader as shpreader # to hide Shapely 2.0 (future) warnings import warnings from shapely.errors import ShapelyDeprecationWarning warnings.filterwarnings(&quot;ignore&quot;, category=ShapelyDeprecationWarning) # Number of Sales per state #df_2 = df.groupby([&#39;State&#39;]).size() # Number of Sales in $ per state df_2 = df.groupby([&#39;State&#39;]).sum(&#39;Sales&#39;)[&#39;Sales&#39;] fig = plt.figure(figsize=(12,8)) min_worth = 1000 # to get the effect of having just the states without a map &quot;background&quot; # turn off the background patch and axes frame ax = fig.add_axes([0, 0, 1, 1], projection=ccrs.LambertConformal(), frameon=False) ax.patch.set_visible(False) ax.set_extent([-125, -66.5, 20, 50], ccrs.Geodetic()) ax.set_title(&#39;US States sales in $ n (red/orange/yellow : less than $1K/2K/3K)&#39;) # load US states boundaries shapename = &#39;admin_1_states_provinces_lakes&#39; states_shp = shpreader.natural_earth(resolution=&#39;110m&#39;, category=&#39;cultural&#39;, name=shapename) # colormap to get colors based on a value (from 0 to max) cmap = mpl.cm.Blues(np.linspace(0, 1, int(df_2.max())+1)) # for all states in the shapefile for state in shpreader.Reader(states_shp).records(): state_name = state.attributes[&#39;name&#39;].rstrip(&#39; x00&#39;) x = state.geometry.centroid.x y = state.geometry.centroid.y try: value = int(df_2[state_name]) except KeyError: value = 0 #print(state_name, value, cmap[value]) if value &lt; min_worth: color = &#39;red&#39; elif value &lt; 2*min_worth: color = &#39;darkorange&#39; elif value &lt; 3*min_worth: color = &#39;yellow&#39; else: color = cmap[value] # add a colored state ax.add_geometries( [state.geometry], ccrs.PlateCarree(), facecolor=color, edgecolor=&#39;black&#39;, #styler=colorize_state #func ) # add its name if(state_name not in [&#39;Hawaii&#39;, &#39;Alaska&#39;]): ax.text( x, y, state_name, color=&#39;black&#39; if value &lt; 300000 else &#39;white&#39;, size=10, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ccrs.PlateCarree(), path_effects=[PathEffects.withStroke(linewidth=1, foreground=&quot;k&quot;, alpha=.8)] ) ax2 = fig.add_axes([0.95,0.10,0.05,0.85]) norm = mpl.colors.Normalize(vmin=0,vmax=df_2.max()+1) cb1 = mpl.colorbar.ColorbarBase(ax2,cmap=mpl.cm.Blues,norm=norm,orientation=&#39;vertical&#39;) plt.show() . . /usr/local/lib/python3.7/dist-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_1_states_provinces_lakes.zip warnings.warn(&#39;Downloading: {}&#39;.format(url), DownloadWarning) . Time series . Visualization of the $ made per day over 4 years . import matplotlib.pyplot as plt #print(df.columns) # Index([&#39;Row ID&#39;, &#39;Order ID&#39;, &#39;Order Date&#39;, Ship Date&#39;, &#39;Ship Mode&#39;, # &#39;Customer ID&#39;, &#39;Customer Name&#39;, &#39;Segment&#39;, &#39;Country&#39;, &#39;City&#39;, # &#39;State&#39;, &#39;Postal Code&#39;, &#39;Region&#39;, &#39;Product ID&#39;, &#39;Category&#39;, # &#39;Sub-Category&#39;, &#39;Product Name&#39;, &#39;Sales&#39;], dtype=&#39;object&#39;) # Convert to panda date df[&#39;Order Date 2&#39;] = pd.to_datetime(df[&#39;Order Date&#39;], format=&#39;%d/%m/%Y&#39;) #print(df[&#39;Order Date 2&#39;]) df.set_index([&#39;Order Date 2&#39;], inplace=True) # reset_index : to keep columns (names) ordered_sales = df.groupby([&#39;Order Date 2&#39;])[&#39;Sales&#39;].sum().reset_index() #print(ordered_sales.columns) # Index([&#39;Sales&#39;], dtype=&#39;object&#39;) #print(ordered_sales) # 1230 lines (days on 4 years) start_date = ordered_sales[&#39;Order Date 2&#39;].min() end_date = ordered_sales[&#39;Order Date 2&#39;].max() #print(start_date, end_date) # 2015-01-03 00:00:00 2018-12-30 00:00:00 #idx = pd.date_range(start=start_date, end=end_date, freq=&#39;D&#39;) # 1458 days : DatetimeIndex([&#39;2015-01-03&#39;, ..., &#39;2018-12-30&#39;]) #ordered_sales[&#39;Order Date&#39;].index = pd.DatetimeIndex(pd.to_datetime(ordered_sales[&#39;Order Date&#39;], format=&#39;%d/%m/%Y&#39;).index) #ordered_sales[&#39;Sales&#39;] = ordered_sales[&#39;Sales&#39;].reindex(idx, fill_value=0) #print(ordered_sales.set_index(&#39;Order Date&#39;)) fig, ax = plt.subplots(figsize=(20,5)) ax.plot(ordered_sales[&#39;Order Date 2&#39;], ordered_sales[&#39;Sales&#39;], color=&#39;green&#39;) ax.set_title(&#39;Sales in $ per day&#39;) ax.set_xlabel(&#39;Date&#39;) ax.set_ylabel(&#39;Dollar ($)&#39;) ax.set_xlim(pd.Timestamp(&#39;2015-01-01&#39;), pd.Timestamp(&#39;2018-12-31&#39;)) plt.show() . . groupings/counts . Various tests with value_counts() &amp; nunique() . test = df.loc[:, [&#39;Category&#39;, &#39;Sub-Category&#39;]]==[&#39;Furniture&#39;, &#39;Bookcases&#39;] print(test.head()) print(&quot;-&quot;*30) print(test.value_counts()) print(&quot;-&quot;*30) print(test.nunique()) . Category Sub-Category Order Date 2 2017-11-08 True True 2017-11-08 True False 2017-06-12 False False 2016-10-11 True False 2016-10-11 False False Category Sub-Category False False 7722 True False 1852 True 226 dtype: int64 Category 2 Sub-Category 2 dtype: int64 . Bookcases . bookcases = df.query(&quot;Category == &#39;Furniture&#39; &amp; `Sub-Category` == &#39;Bookcases&#39;&quot;) # filter on 2 columns (Logical AND) print(&quot;Filtered dataset&#39;s size :&quot;, bookcases.shape) # 226 rows print(&quot;-&quot;*10+&quot; counts of distinct values &quot;+&quot;-&quot;*10) print(bookcases[&#39;Ship Mode&#39;].value_counts()) print(&quot;-&quot;*10+&quot;&quot;+&quot;-&quot;*10) print(&quot;Number of distinct values :&quot;, bookcases[&#39;Ship Mode&#39;].nunique()) # all of them 4 are present . Filtered dataset&#39;s size : (226, 22) - counts of distinct values - Standard Class 123 Second Class 48 First Class 48 Same Day 7 Name: Ship Mode, dtype: int64 -- Number of distinct values : 4 . Contingency table (crosstab) : for bookcases shipping to California (and outside) . California customer seems to be using a bit less standard shipping on average (?maybe) | https://en.wikipedia.org/wiki/Contingency_table | . test = bookcases.loc[:, [&#39;Ship Mode&#39;, &#39;State&#39;]]==[&#39;Standard Class&#39;, &#39;California&#39;] cnt = test[[&#39;Ship Mode&#39;, &#39;State&#39;]].value_counts().reset_index() #print(cnt) pd.crosstab(test[&#39;State&#39;], test[&#39;Ship Mode&#39;], rownames=[&#39;From California&#39;], colnames=[&#39;Is Standard Class ?&#39;], margins=True) . Is Standard Class ? False True All . From California . False 79 | 96 | 175 | . True 24 | 27 | 51 | . All 103 | 123 | 226 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pd.crosstab(test[&#39;State&#39;], test[&#39;Ship Mode&#39;], rownames=[&#39;From California&#39;], colnames=[&#39;Is Standard Class ?&#39;], margins=True, normalize=&#39;index&#39;) . Is Standard Class ? False True . From California . False 0.451429 | 0.548571 | . True 0.470588 | 0.529412 | . All 0.455752 | 0.544248 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(&quot;-&quot;*30) print(bookcases[[&#39;Ship Mode&#39;, &#39;State&#39;]].value_counts()) print(&quot;-&quot;*30) print(bookcases.nunique()) . Ship Mode State Standard Class California 27 New York 18 Texas 16 First Class California 11 Second Class California 10 .. Missouri 1 Same Day Connecticut 1 Second Class Virginia 1 Utah 1 First Class South Dakota 1 Length: 67, dtype: int64 Row ID 226 Order ID 222 Order Date 199 Ship Date 209 Ship Mode 4 Customer ID 194 Customer Name 194 Segment 3 Country 1 City 108 State 33 Postal Code 136 Region 4 Product ID 49 Category 1 Sub-Category 1 Product Name 50 Sales 196 month 12 year 4 Duration 8 DoW 7 dtype: int64 . Count products ordered by corporations in California . print(df.query(&quot;State == &#39;California&#39; &amp; Segment == &#39;Corporate&#39;&quot;).shape) # (601 rows, 18 feature original columns) . (601, 22) . Correlations . There is no correlation between Sales and shipping duration . df[&quot;Duration&quot;] = (pd.to_datetime(df[&quot;Ship Date&quot;], format=&#39;%d/%m/%Y&#39;) - pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;)).astype(&#39;timedelta64[h]&#39;).astype(np.int32) / 24 fig, ax = plt.subplots(1,2, figsize=(10,5)) ax[0].set_title(&#39;Pearson correlation coefficient&#39;) sns.heatmap( df[[&#39;Sales&#39;, &#39;Duration&#39;]].corr(), # method=&#39;pearson&#39; (by default) cmap=&quot;YlGnBu&quot;, annot=True, fmt=&quot;f&quot;, cbar=False, ax=ax[0] ) # =&gt; 0.005712 ax[1].set_title(r&quot;Spearman&#39;s $ rho$ (rank correlation)&quot;) sns.heatmap( df[[&#39;Sales&#39;, &#39;Duration&#39;]].corr(method=&#39;spearman&#39;), # for non-linear correlation... cmap=&quot;YlGnBu&quot;, annot=True, fmt=&quot;f&quot;, cbar=False, ax=ax[1] ); # abs = 0.014283 (not relevant, extremely small inverse correlation) . Various charts (again) . The duration between the day the order is taken and its shipping (expedition?) doesn&#39;t seem to be correlated... . import seaborn as sns sns.set_theme(style=&quot;whitegrid&quot;) cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True) df[&quot;Duration&quot;] = (pd.to_datetime(df[&quot;Ship Date&quot;], format=&#39;%d/%m/%Y&#39;) - pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;)).astype(&#39;timedelta64[h]&#39;).astype(np.int32) / 24 g = sns.relplot( data=df[[&#39;Sales&#39;, &#39;Duration&#39;]], x=&quot;Sales&quot;, y=&quot;Duration&quot;, #hue=&quot;year&quot;, size=&quot;mass&quot;, palette=cmap, sizes=(10, 200), ) g.set(xscale=&quot;log&quot;) g.ax.xaxis.grid(True, &quot;minor&quot;, linewidth=.25) g.ax.yaxis.grid(True, &quot;minor&quot;, linewidth=.25) g.despine(left=True, bottom=True); . Plot univariate (or bivariate distributions) using kernel density estimation https://seaborn.pydata.org/generated/seaborn.kdeplot.html . #df.reset_index(inplace=True) # ... sns.kdeplot(data=df, x=&#39;Sales&#39;); . sns.kdeplot(df[&#39;Duration&#39;], cumulative=True); . sns.displot(df[&#39;Sales&#39;], color=&quot;g&quot;); . Pair plot : &quot;pairwise relationships&quot; of numerical columns, except 2 columns . sns.pairplot(df.loc[:, ~df.columns.isin([&#39;Row ID&#39;, &#39;Postal Code&#39;])]); . Times from ORDER to SHIPPING . ax = sns.histplot(df[[&#39;Duration&#39;]], kde=True) # divided in 100 parts (=bins) ax.set_title(&#39;Time taken between Order and Shipping (in days)&#39;); . Distribution of sales in dollar . plt.figure(figsize=(25,5)) sns.histplot(df[[&#39;Sales&#39;]], bins=150) # divided in 100 parts (=bins) ax.set_title(&#39;Number of sales&#39;); . Most orders are treated in 4 days are we see on this bi-variate histogram (with a log y-axis) . plt.figure(figsize=(12,5)) #df.reset_index(inplace=True) # to prevent &quot;cannot reindex from a duplicate axis&quot; error ??? sns.histplot( df, x=&quot;Duration&quot;, y=&quot;Sales&quot;, hue=&#39;Ship Mode&#39;, bins=30, discrete=(True, False), log_scale=(False, True), #cbar=True, cbar_kws=dict(shrink=.75), ); . c1 = df[&#39;State&#39;].where((df[&#39;State&#39;] == &#39;California&#39;) | (df[&#39;State&#39;] == &#39;New York&#39;)) c2 = df.Duration.isin([0, 1, 2]) # Fast ? # Summary table : sns.heatmap(pd.crosstab( c1, c2, #normalize=True, # rows sum to 1 margins=True, margins_name=&#39;Total&#39; ), cmap=&quot;YlGnBu&quot;, annot=True, fmt=&quot;d&quot;, cbar=False) df_test = pd.concat([c1, c2], axis=1, keys=[&#39;State&#39;, &#39;isFast&#39;]) # same results / other method df_test.groupby([&#39;State&#39;, &#39;isFast&#39;])[&#39;State&#39;].count().unstack().fillna(0) . isFast False True . State . California 1464 | 482 | . New York 869 | 228 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df[[&#39;Sales&#39;, &#39;Duration&#39;]].cov() . Sales Duration . Sales 392692.572239 | -6.263013 | . Duration -6.263013 | 3.061148 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt;",
            "url": "https://notebooks.yannick42.dev/eda/2022/04/22/Superstore_Sales_Dataset_(EDA).html",
            "relUrl": "/eda/2022/04/22/Superstore_Sales_Dataset_(EDA).html",
            "date": " • Apr 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Kaggle Survey 2020 (WIP)",
            "content": ". An industry-wide survey, and comprehensive view of the state of data science and machine learning. https://www.kaggle.com/competitions/kaggle-survey-2020 . Setup . %%capture !pip install kaggle --upgrade . import os from getpass import getpass kaggle_username = input(&quot;Kaggle USERNAME : &quot;) os.environ[&#39;KAGGLE_USERNAME&#39;] = kaggle_username kaggle_key = getpass(&quot;Kaggle KEY for &quot;+os.environ.get(&#39;KAGGLE_USERNAME&#39;)+&quot;: &quot;) os.environ[&quot;KAGGLE_KEY&quot;] = kaggle_key . Kaggle USERNAME : yannick42 Kaggle KEY for yannick42: ·········· . !kaggle competitions download -c kaggle-survey-2020 !unzip -n kaggle-survey-2020.zip !ls -la . Downloading kaggle-survey-2020.zip to /content 0% 0.00/2.08M [00:00&lt;?, ?B/s] 100% 2.08M/2.08M [00:00&lt;00:00, 137MB/s] Archive: kaggle-survey-2020.zip inflating: kaggle_survey_2020_responses.csv inflating: supplementary_data/kaggle_survey_2020_answer_choices.pdf inflating: supplementary_data/kaggle_survey_2020_methodology.pdf total 26988 drwxr-xr-x 1 root root 4096 Apr 22 20:02 . drwxr-xr-x 1 root root 4096 Apr 22 18:21 .. drwxr-xr-x 4 root root 4096 Apr 19 14:22 .config -rw-r--r-- 1 root root 25431748 Nov 17 2020 kaggle_survey_2020_responses.csv -rw-r--r-- 1 root root 2179679 Apr 22 20:02 kaggle-survey-2020.zip drwxr-xr-x 1 root root 4096 Apr 19 14:23 sample_data drwxr-xr-x 2 root root 4096 Apr 22 20:02 supplementary_data . Load &amp; display some data . import pandas as pd import numpy as np np.random.seed(42) from matplotlib import pyplot as plt import seaborn as sns &quot;&quot;&quot; x columns : - questions &quot;&quot;&quot; # header=None =&gt; skip header... (but no more &quot;Q1&quot;, ...) df = pd.read_csv( &#39;kaggle_survey_2020_responses.csv&#39;, parse_dates=True, infer_datetime_format=True ) . /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) . print(len(df.columns.to_list()) - 1, &quot;questions&quot;) df.describe() . 354 questions . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 ... Q35_B_Part_2 Q35_B_Part_3 Q35_B_Part_4 Q35_B_Part_5 Q35_B_Part_6 Q35_B_Part_7 Q35_B_Part_8 Q35_B_Part_9 Q35_B_Part_10 Q35_B_OTHER . count 20037 | 20037 | 20037 | 20037 | 19570 | 19278 | 19121 | 15531 | 4278 | 7536 | ... | 1178 | 495 | 431 | 3200 | 558 | 481 | 847 | 520 | 3083 | 252 | . unique 5168 | 12 | 6 | 56 | 8 | 14 | 8 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . top 565 | 25-29 | Man | India | Master’s degree | Student | 3-5 years | Python | R | SQL | ... | Weights &amp; Biases | Comet.ml | Sacred + Omniboard | TensorBoard | Guild.ai | Polyaxon | Trains | Domino Model Monitor | None | Other | . freq 34 | 4011 | 15789 | 5851 | 7859 | 5171 | 4546 | 15530 | 4277 | 7535 | ... | 1177 | 494 | 430 | 3199 | 557 | 480 | 846 | 519 | 3082 | 251 | . 4 rows × 355 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.head() . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 ... Q35_B_Part_2 Q35_B_Part_3 Q35_B_Part_4 Q35_B_Part_5 Q35_B_Part_6 Q35_B_Part_7 Q35_B_Part_8 Q35_B_Part_9 Q35_B_Part_10 Q35_B_OTHER . 0 Duration (in seconds) | What is your age (# years)? | What is your gender? - Selected Choice | In which country do you currently reside? | What is the highest level of formal education ... | Select the title most similar to your current ... | For how many years have you been writing code ... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | ... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | . 1 1838 | 35-39 | Man | Colombia | Doctoral degree | Student | 5-10 years | Python | R | SQL | ... | NaN | NaN | NaN | TensorBoard | NaN | NaN | NaN | NaN | NaN | NaN | . 2 289287 | 30-34 | Man | United States of America | Master’s degree | Data Engineer | 5-10 years | Python | R | SQL | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 860 | 35-39 | Man | Argentina | Bachelor’s degree | Software Engineer | 10-20 years | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | None | NaN | . 4 507 | 30-34 | Man | United States of America | Master’s degree | Data Scientist | 5-10 years | Python | NaN | SQL | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 355 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Pie charts . # Question 2 s = df.groupby([&#39;Q1&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q1: What is your age ?&#39;) # Question 2 s = df.groupby([&#39;Q2&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q2: What is your gender ?&#39;) # Question 3 s = df.groupby([&#39;Q3&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q3: What is your country ?&#39;) # Question 4 s = df.groupby([&#39;Q4&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q4: What is the highest level of formal education that you have attained or plan to attain within the next 2 years ?&#39;) print() . . .",
            "url": "https://notebooks.yannick42.dev/eda/2022/04/22/Kaggle_Survey_2020.html",
            "relUrl": "/eda/2022/04/22/Kaggle_Survey_2020.html",
            "date": " • Apr 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://notebooks.yannick42.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://notebooks.yannick42.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}