{
  
    
        "post0": {
            "title": "Superstore Sales Dataset (WIP)",
            "content": ". . Note: This is a test . . Warning: This is a Work in Progress Superstore Sales Dataset : https://www.kaggle.com/datasets/rohitsahoo/sales-forecasting . TODO : . find correlations... | try to use an application of PCA (?) | . Setup . %%capture !pip install kaggle --upgrade . %%capture import os from getpass import getpass kaggle_username = input(&quot;Kaggle USERNAME : &quot;) os.environ[&#39;KAGGLE_USERNAME&#39;] = kaggle_username kaggle_key = getpass(&quot;Kaggle KEY for &quot;+os.environ.get(&#39;KAGGLE_USERNAME&#39;)+&quot;: &quot;) os.environ[&quot;KAGGLE_KEY&quot;] = kaggle_key . Kaggle USERNAME : yannick42 Kaggle KEY for yannick42: ·········· . !kaggle datasets download -d rohitsahoo/sales-forecasting !unzip -n sales-forecasting.zip !ls -la . Downloading sales-forecasting.zip to /content 0% 0.00/480k [00:00&lt;?, ?B/s] 100% 480k/480k [00:00&lt;00:00, 24.6MB/s] Archive: sales-forecasting.zip inflating: train.csv total 2580 drwxr-xr-x 1 root root 4096 Apr 22 20:16 . drwxr-xr-x 1 root root 4096 Apr 22 20:14 .. drwxr-xr-x 4 root root 4096 Apr 19 14:22 .config -rw-r--r-- 1 root root 491942 Apr 22 20:16 sales-forecasting.zip drwxr-xr-x 1 root root 4096 Apr 19 14:23 sample_data -rw-r--r-- 1 root root 2129689 Sep 11 2020 train.csv . Load &amp; Display some data . %matplotlib inline import pandas as pd import numpy as np np.random.seed(42) from matplotlib import pyplot as plt import seaborn as sns &quot;&quot;&quot; 18 columns : - 1 numeric - sales (in $) - 14 strings - order date - ship date - category - sub-category - ... &quot;&quot;&quot; df = pd.read_csv(&#39;train.csv&#39;, parse_dates=True, infer_datetime_format=True) . &quot;&quot;&quot; - TODO : Postal Code could be filled by using the city name and using what other lines with the same city name uses ... ? (-&gt; only 11 rows have this problem on 9800 sales) &quot;&quot;&quot; df[&#39;Sales&#39;].describe() . count 9800.000000 mean 230.769059 std 626.651875 min 0.444000 25% 17.248000 50% 54.490000 75% 210.605000 max 22638.480000 Name: Sales, dtype: float64 . df.head() # first 5 rows . Row ID Order ID Order Date Ship Date Ship Mode Customer ID Customer Name Segment Country City State Postal Code Region Product ID Category Sub-Category Product Name Sales . 0 1 | CA-2017-152156 | 08/11/2017 | 11/11/2017 | Second Class | CG-12520 | Claire Gute | Consumer | United States | Henderson | Kentucky | 42420.0 | South | FUR-BO-10001798 | Furniture | Bookcases | Bush Somerset Collection Bookcase | 261.9600 | . 1 2 | CA-2017-152156 | 08/11/2017 | 11/11/2017 | Second Class | CG-12520 | Claire Gute | Consumer | United States | Henderson | Kentucky | 42420.0 | South | FUR-CH-10000454 | Furniture | Chairs | Hon Deluxe Fabric Upholstered Stacking Chairs,... | 731.9400 | . 2 3 | CA-2017-138688 | 12/06/2017 | 16/06/2017 | Second Class | DV-13045 | Darrin Van Huff | Corporate | United States | Los Angeles | California | 90036.0 | West | OFF-LA-10000240 | Office Supplies | Labels | Self-Adhesive Address Labels for Typewriters b... | 14.6200 | . 3 4 | US-2016-108966 | 11/10/2016 | 18/10/2016 | Standard Class | SO-20335 | Sean O&#39;Donnell | Consumer | United States | Fort Lauderdale | Florida | 33311.0 | South | FUR-TA-10000577 | Furniture | Tables | Bretford CR4500 Series Slim Rectangular Table | 957.5775 | . 4 5 | US-2016-108966 | 11/10/2016 | 18/10/2016 | Standard Class | SO-20335 | Sean O&#39;Donnell | Consumer | United States | Fort Lauderdale | Florida | 33311.0 | South | OFF-ST-10000760 | Office Supplies | Storage | Eldon Fold &#39;N Roll Cart System | 22.3680 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Various Charts... . The boxplot shows the sales (in $) distribution is skewed. There is a lot of &quot;outliers&quot; (higher amount orders), but the median is $54 . plt.figure(figsize=(25,5)) ax = sns.boxplot( y=&#39;Segment&#39;, x=&#39;Sales&#39;, orient=&quot;h&quot;, # horizontal data=df.query(&#39;Segment == &quot;Consumer&quot; | Segment == &quot;Corporate&quot; | Segment == &quot;Home Office&quot;&#39;), showmeans=True, showfliers=True, # (default=None) False to remove outliers flierprops=dict( marker=&#39;x&#39;, markerfacecolor=None, markersize=6, markeredgecolor=&#39;red&#39; ) ) ax.set(xlabel=&#39;Sales in $&#39;, xlim=(0, None)) plt.show() . . The number of sales seems to be increasing each year . plt.figure(figsize=(14,6)) # transform &quot;string&quot; to date &quot;number&quot; df[&#39;month&#39;] = pd.to_datetime(df[&#39;Order Date&#39;]).dt.month df[&#39;year&#39;] = pd.to_datetime(df[&#39;Order Date&#39;]).dt.to_period(&#39;Y&#39;) #print(df.month.head()) #print(df.year.head()) df_grouped = df.groupby([&#39;month&#39;, &#39;year&#39;]).size().to_frame(&#39;size&#39;) # to name the size column... &#39;cause size() returns a Series object total_sales = df_grouped[&#39;size&#39;].sum() #print(df_grouped) piv = pd.pivot_table(df_grouped, values=&quot;size&quot;,index=[&quot;year&quot;], columns=[&quot;month&quot;], fill_value=0) sns.heatmap(piv, xticklabels=[&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;], annot=True, fmt=&quot;d&quot;, linewidths=0.5, cmap=&quot;Blues&quot;); . . September, and the end of the year seems to be the most profitable months of the year . plt.figure(figsize=(14,6)) # generated in the cell above df_grouped_dollar = df.groupby([&#39;month&#39;, &#39;year&#39;]).sum(&#39;Sales&#39;) piv_dollar = pd.pivot_table(df_grouped_dollar, values=&quot;Sales&quot;,index=[&quot;year&quot;], columns=[&quot;month&quot;], fill_value=0) ax = sns.heatmap(piv_dollar, xticklabels=[&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;], annot=True, fmt=&quot;g&quot;, linewidths=1.5, cmap=&quot;Greens&quot;) ax.set_title(&#39;Sales in $&#39;) total_sales_in_dollar = df_grouped_dollar[&#39;Sales&#39;].sum() print(&quot;Total value over 4 years : &quot;+str(round(total_sales_in_dollar / 1000 / 1000, 3))+&quot; M$&quot;) print(&quot;Mean sale value : &quot;+str(round(total_sales_in_dollar/total_sales))+&quot;$&quot;) plt.show() . . Total value over 4 years : 2.262 M$ Mean sale value : 231$ . US Map . %%capture !pip install --upgrade pyshp !pip install cartopy==0.19.0.post1 !pip uninstall -y shapely # to prevent this error : https://stackoverflow.com/questions/60111684/geometry-must-be-a-point-or-linestring-error-using-cartopy !pip install shapely --no-binary shapely . A few states represents most of the sales (in $) . import matplotlib.patches as mpatches import matplotlib.pyplot as plt import matplotlib as mpl import shapely.geometry as sgeom import cartopy.crs as ccrs import cartopy.io.shapereader as shpreader # to hide Shapely 2.0 (future) warnings import warnings from shapely.errors import ShapelyDeprecationWarning warnings.filterwarnings(&quot;ignore&quot;, category=ShapelyDeprecationWarning) # Number of Sales per state #df_2 = df.groupby([&#39;State&#39;]).size() # Number of Sales in $ per state df_2 = df.groupby([&#39;State&#39;]).sum(&#39;Sales&#39;)[&#39;Sales&#39;] fig = plt.figure(figsize=(12,8)) min_worth = 1000 # to get the effect of having just the states without a map &quot;background&quot; # turn off the background patch and axes frame ax = fig.add_axes([0, 0, 1, 1], projection=ccrs.LambertConformal(), frameon=False) ax.patch.set_visible(False) ax.set_extent([-125, -66.5, 20, 50], ccrs.Geodetic()) ax.set_title(&#39;US States sales in $ n (red/orange/yellow : less than $1K/2K/3K)&#39;) # load US states boundaries shapename = &#39;admin_1_states_provinces_lakes&#39; states_shp = shpreader.natural_earth(resolution=&#39;110m&#39;, category=&#39;cultural&#39;, name=shapename) # colormap to get colors based on a value (from 0 to max) cmap = mpl.cm.Blues(np.linspace(0, 1, int(df_2.max())+1)) # for all states in the shapefile for state in shpreader.Reader(states_shp).records(): state_name = state.attributes[&#39;name&#39;].rstrip(&#39; x00&#39;) try: value = int(df_2[state_name]) except KeyError: value = 0 #print(state_name, value, cmap[value]) if value &lt; min_worth: color = &#39;red&#39; elif value &lt; 2*min_worth: color = &#39;darkorange&#39; elif value &lt; 3*min_worth: color = &#39;yellow&#39; else: color = cmap[value] # add a colored state ax.add_geometries( [state.geometry], ccrs.PlateCarree(), facecolor=color, edgecolor=&#39;black&#39;, #styler=colorize_state #func ) ax2 = fig.add_axes([0.95,0.10,0.05,0.85]) norm = mpl.colors.Normalize(vmin=0,vmax=df_2.max()+1) cb1 = mpl.colorbar.ColorbarBase(ax2,cmap=mpl.cm.Blues,norm=norm,orientation=&#39;vertical&#39;) plt.show() . . /usr/local/lib/python3.7/dist-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_1_states_provinces_lakes.zip warnings.warn(&#39;Downloading: {}&#39;.format(url), DownloadWarning) . Time serie . Visualization of the $ made per day over 4 years . import matplotlib.pyplot as plt #print(df.columns) # Index([&#39;Row ID&#39;, &#39;Order ID&#39;, &#39;Order Date&#39;, Ship Date&#39;, &#39;Ship Mode&#39;, # &#39;Customer ID&#39;, &#39;Customer Name&#39;, &#39;Segment&#39;, &#39;Country&#39;, &#39;City&#39;, # &#39;State&#39;, &#39;Postal Code&#39;, &#39;Region&#39;, &#39;Product ID&#39;, &#39;Category&#39;, # &#39;Sub-Category&#39;, &#39;Product Name&#39;, &#39;Sales&#39;], dtype=&#39;object&#39;) # Convert to panda date df[&#39;Order Date 2&#39;] = pd.to_datetime(df[&#39;Order Date&#39;], format=&#39;%d/%m/%Y&#39;) #print(df[&#39;Order Date 2&#39;]) df.set_index([&#39;Order Date 2&#39;], inplace=True) # reset_index : to keep columns (names) ordered_sales = df.groupby([&#39;Order Date 2&#39;])[&#39;Sales&#39;].sum().reset_index() #print(ordered_sales.columns) # Index([&#39;Sales&#39;], dtype=&#39;object&#39;) #print(ordered_sales) # 1230 lines (days on 4 years) start_date = ordered_sales[&#39;Order Date 2&#39;].min() end_date = ordered_sales[&#39;Order Date 2&#39;].max() #print(start_date, end_date) # 2015-01-03 00:00:00 2018-12-30 00:00:00 #idx = pd.date_range(start=start_date, end=end_date, freq=&#39;D&#39;) # 1458 days : DatetimeIndex([&#39;2015-01-03&#39;, ..., &#39;2018-12-30&#39;]) #ordered_sales[&#39;Order Date&#39;].index = pd.DatetimeIndex(pd.to_datetime(ordered_sales[&#39;Order Date&#39;], format=&#39;%d/%m/%Y&#39;).index) #ordered_sales[&#39;Sales&#39;] = ordered_sales[&#39;Sales&#39;].reindex(idx, fill_value=0) #print(ordered_sales.set_index(&#39;Order Date&#39;)) #plt.figure(figsize=(20,5)) fig, ax = plt.subplots(figsize=(20,5)) ax.plot(ordered_sales[&#39;Order Date 2&#39;], ordered_sales[&#39;Sales&#39;]) ax.set_title(&#39;Sales in $ per day&#39;) ax.set_xlabel(&#39;Date&#39;) ax.set_ylabel(&#39;Dollar ($)&#39;) ax.set_xlim(pd.Timestamp(&#39;2015-01-01&#39;), pd.Timestamp(&#39;2018-12-31&#39;)) plt.show() . . Various charts (again) . import seaborn as sns sns.set_theme(style=&quot;whitegrid&quot;) cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True) df[&quot;Duration&quot;] = (pd.to_datetime(df[&quot;Ship Date&quot;], format=&#39;%d/%m/%Y&#39;) - pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;)).astype(&#39;timedelta64[h]&#39;).astype(np.int32) / 24 g = sns.relplot( data=df[[&#39;Sales&#39;, &#39;Duration&#39;]], x=&quot;Sales&quot;, y=&quot;Duration&quot;, #hue=&quot;year&quot;, size=&quot;mass&quot;, palette=cmap, sizes=(10, 200), ) g.set(xscale=&quot;log&quot;) g.ax.xaxis.grid(True, &quot;minor&quot;, linewidth=.25) g.ax.yaxis.grid(True, &quot;minor&quot;, linewidth=.25) g.despine(left=True, bottom=True); . test = df.loc[:, [&#39;Category&#39;, &#39;Sub-Category&#39;]]==[&#39;Furniture&#39;, &#39;Bookcases&#39;] print(test.head()) print(&quot;-&quot;*30) print(test.value_counts()) print(&quot;-&quot;*30) print(test.nunique()) . Category Sub-Category Order Date 2 2017-11-08 True True 2017-11-08 True False 2017-06-12 False False 2016-10-11 True False 2016-10-11 False False Category Sub-Category False False 7722 True False 1852 True 226 dtype: int64 Category 2 Sub-Category 2 dtype: int64 . bookcases = df.query(&quot;Category == &#39;Furniture&#39; &amp; `Sub-Category` == &#39;Bookcases&#39;&quot;) # filter on 2 columns (Logical AND) print(&quot;Filtered dataset&#39;s size :&quot;, bookcases.shape) # (226 rows, 18 columns) print(&quot;-&quot;*10+&quot; counts of distinct values &quot;+&quot;-&quot;*10) print(bookcases[&#39;Ship Mode&#39;].value_counts()) print(&quot;-&quot;*10+&quot;&quot;+&quot;-&quot;*10) print(&quot;Number of distinct values :&quot;, bookcases[&#39;Ship Mode&#39;].nunique()) . Filtered dataset&#39;s size : (226, 21) - counts of distinct values - Standard Class 123 Second Class 48 First Class 48 Same Day 7 Name: Ship Mode, dtype: int64 -- Number of distinct values : 4 . test = bookcases.loc[:, [&#39;Ship Mode&#39;, &#39;State&#39;]]==[&#39;Standard Class&#39;, &#39;California&#39;] #print(test) print(test[[&#39;Ship Mode&#39;, &#39;State&#39;]].value_counts()) . Ship Mode State True False 96 False False 79 True True 27 False True 24 dtype: int64 . print(&quot;-&quot;*30) print(bookcases[[&#39;Ship Mode&#39;, &#39;State&#39;]].value_counts()) print(&quot;-&quot;*30) print(bookcases.nunique()) . Ship Mode State Standard Class California 27 New York 18 Texas 16 First Class California 11 Second Class California 10 .. Missouri 1 Same Day Connecticut 1 Second Class Virginia 1 Utah 1 First Class South Dakota 1 Length: 67, dtype: int64 Row ID 226 Order ID 222 Order Date 199 Ship Date 209 Ship Mode 4 Customer ID 194 Customer Name 194 Segment 3 Country 1 City 108 State 33 Postal Code 136 Region 4 Product ID 49 Category 1 Sub-Category 1 Product Name 50 Sales 196 month 12 year 4 Duration 8 dtype: int64 . print(df.query(&quot;State == &#39;California&#39; &amp; Segment == &#39;Corporate&#39;&quot;).shape) # (601 rows, 18 feature columns) . (601, 21) . CORRELATIONs . df[&quot;Duration&quot;] = (pd.to_datetime(df[&quot;Ship Date&quot;], format=&#39;%d/%m/%Y&#39;) - pd.to_datetime(df[&quot;Order Date&quot;], format=&#39;%d/%m/%Y&#39;)).astype(&#39;timedelta64[h]&#39;).astype(np.int32) / 24 print(df[[&#39;Sales&#39;, &#39;Duration&#39;]].corr()) # method=&#39;pearson&#39; (by default) # =&gt; 0.005712 print(df[[&#39;Sales&#39;, &#39;Duration&#39;]].corr(method=&#39;spearman&#39;)) # for non-linear correlation... # =&gt; 0.014283 . Sales Duration Sales 1.000000 -0.005712 Duration -0.005712 1.000000 Sales Duration Sales 1.000000 -0.014283 Duration -0.014283 1.000000 . Plot univariate (or bivariate distributions) using kernel density estimation https://seaborn.pydata.org/generated/seaborn.kdeplot.html . print(df.columns) df.reset_index(inplace=True) sns.kdeplot(data=df, x=&#39;Sales&#39;); . Index([&#39;Row ID&#39;, &#39;Order ID&#39;, &#39;Order Date&#39;, &#39;Ship Date&#39;, &#39;Ship Mode&#39;, &#39;Customer ID&#39;, &#39;Customer Name&#39;, &#39;Segment&#39;, &#39;Country&#39;, &#39;City&#39;, &#39;State&#39;, &#39;Postal Code&#39;, &#39;Region&#39;, &#39;Product ID&#39;, &#39;Category&#39;, &#39;Sub-Category&#39;, &#39;Product Name&#39;, &#39;Sales&#39;, &#39;month&#39;, &#39;year&#39;, &#39;Duration&#39;], dtype=&#39;object&#39;) . sns.kdeplot(df[&#39;Duration&#39;], cumulative=True); . sns.displot(df[&#39;Sales&#39;], color=&quot;g&quot;); . sns.pairplot(df[[&#39;Sales&#39;, &#39;Duration&#39;]]); . sns.histplot(data=df, x=&#39;Sales&#39;, y=&#39;Duration&#39;, hue=&#39;Ship Mode&#39;); . sns.histplot(df[[&#39;Duration&#39;]], kde=True); # divided in 100 parts (=bins) . plt.figure(figsize=(25,5)) sns.histplot(df[[&#39;Sales&#39;]], bins=100); # divided in 100 parts (=bins) . c1 = df[&#39;State&#39;].where((df[&#39;State&#39;] == &#39;California&#39;) | (df[&#39;State&#39;] == &#39;New York&#39;)) c2 = df.Duration.isin([0, 1, 2]) # Fast ? # Summary table : sns.heatmap(pd.crosstab( c1, c2, #normalize=True, # rows sum to 1 margins=True, margins_name=&#39;Total&#39; ), cmap=&quot;YlGnBu&quot;, annot=True, fmt=&quot;d&quot;, cbar=False) df_test = pd.concat([c1, c2], axis=1, keys=[&#39;State&#39;, &#39;isFast&#39;]) # same results / other method df_test.groupby([&#39;State&#39;, &#39;isFast&#39;])[&#39;State&#39;].count().unstack().fillna(0) . isFast False True . State . California 1464 | 482 | . New York 869 | 228 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df[[&#39;Sales&#39;, &#39;Duration&#39;]].cov() . Sales Duration . Sales 392692.572239 | -6.263013 | . Duration -6.263013 | 3.061148 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt;",
            "url": "https://yannick42.github.io/notebooks/eda/2022/04/22/Superstore_Sales_Dataset_(EDA).html",
            "relUrl": "/eda/2022/04/22/Superstore_Sales_Dataset_(EDA).html",
            "date": " • Apr 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Kaggle Survey 2020 (WIP)",
            "content": ". An industry-wide survey, and comprehensive view of the state of data science and machine learning. https://www.kaggle.com/competitions/kaggle-survey-2020 . Setup . %%capture !pip install kaggle --upgrade . import os from getpass import getpass kaggle_username = input(&quot;Kaggle USERNAME : &quot;) os.environ[&#39;KAGGLE_USERNAME&#39;] = kaggle_username kaggle_key = getpass(&quot;Kaggle KEY for &quot;+os.environ.get(&#39;KAGGLE_USERNAME&#39;)+&quot;: &quot;) os.environ[&quot;KAGGLE_KEY&quot;] = kaggle_key . Kaggle USERNAME : yannick42 Kaggle KEY for yannick42: ·········· . !kaggle competitions download -c kaggle-survey-2020 !unzip -n kaggle-survey-2020.zip !ls -la . Downloading kaggle-survey-2020.zip to /content 0% 0.00/2.08M [00:00&lt;?, ?B/s] 100% 2.08M/2.08M [00:00&lt;00:00, 137MB/s] Archive: kaggle-survey-2020.zip inflating: kaggle_survey_2020_responses.csv inflating: supplementary_data/kaggle_survey_2020_answer_choices.pdf inflating: supplementary_data/kaggle_survey_2020_methodology.pdf total 26988 drwxr-xr-x 1 root root 4096 Apr 22 20:02 . drwxr-xr-x 1 root root 4096 Apr 22 18:21 .. drwxr-xr-x 4 root root 4096 Apr 19 14:22 .config -rw-r--r-- 1 root root 25431748 Nov 17 2020 kaggle_survey_2020_responses.csv -rw-r--r-- 1 root root 2179679 Apr 22 20:02 kaggle-survey-2020.zip drwxr-xr-x 1 root root 4096 Apr 19 14:23 sample_data drwxr-xr-x 2 root root 4096 Apr 22 20:02 supplementary_data . Load &amp; display some data . import pandas as pd import numpy as np np.random.seed(42) from matplotlib import pyplot as plt import seaborn as sns &quot;&quot;&quot; x columns : - questions &quot;&quot;&quot; # header=None =&gt; skip header... (but no more &quot;Q1&quot;, ...) df = pd.read_csv( &#39;kaggle_survey_2020_responses.csv&#39;, parse_dates=True, infer_datetime_format=True ) . /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False. exec(code_obj, self.user_global_ns, self.user_ns) . print(len(df.columns.to_list()) - 1, &quot;questions&quot;) df.describe() . 354 questions . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 ... Q35_B_Part_2 Q35_B_Part_3 Q35_B_Part_4 Q35_B_Part_5 Q35_B_Part_6 Q35_B_Part_7 Q35_B_Part_8 Q35_B_Part_9 Q35_B_Part_10 Q35_B_OTHER . count 20037 | 20037 | 20037 | 20037 | 19570 | 19278 | 19121 | 15531 | 4278 | 7536 | ... | 1178 | 495 | 431 | 3200 | 558 | 481 | 847 | 520 | 3083 | 252 | . unique 5168 | 12 | 6 | 56 | 8 | 14 | 8 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . top 565 | 25-29 | Man | India | Master’s degree | Student | 3-5 years | Python | R | SQL | ... | Weights &amp; Biases | Comet.ml | Sacred + Omniboard | TensorBoard | Guild.ai | Polyaxon | Trains | Domino Model Monitor | None | Other | . freq 34 | 4011 | 15789 | 5851 | 7859 | 5171 | 4546 | 15530 | 4277 | 7535 | ... | 1177 | 494 | 430 | 3199 | 557 | 480 | 846 | 519 | 3082 | 251 | . 4 rows × 355 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df.head() . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 ... Q35_B_Part_2 Q35_B_Part_3 Q35_B_Part_4 Q35_B_Part_5 Q35_B_Part_6 Q35_B_Part_7 Q35_B_Part_8 Q35_B_Part_9 Q35_B_Part_10 Q35_B_OTHER . 0 Duration (in seconds) | What is your age (# years)? | What is your gender? - Selected Choice | In which country do you currently reside? | What is the highest level of formal education ... | Select the title most similar to your current ... | For how many years have you been writing code ... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | ... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | . 1 1838 | 35-39 | Man | Colombia | Doctoral degree | Student | 5-10 years | Python | R | SQL | ... | NaN | NaN | NaN | TensorBoard | NaN | NaN | NaN | NaN | NaN | NaN | . 2 289287 | 30-34 | Man | United States of America | Master’s degree | Data Engineer | 5-10 years | Python | R | SQL | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 860 | 35-39 | Man | Argentina | Bachelor’s degree | Software Engineer | 10-20 years | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | None | NaN | . 4 507 | 30-34 | Man | United States of America | Master’s degree | Data Scientist | 5-10 years | Python | NaN | SQL | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 355 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Pie charts . # Question 2 s = df.groupby([&#39;Q1&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q1: What is your age ?&#39;) # Question 2 s = df.groupby([&#39;Q2&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q2: What is your gender ?&#39;) # Question 3 s = df.groupby([&#39;Q3&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q3: What is your country ?&#39;) # Question 4 s = df.groupby([&#39;Q4&#39;]).size().to_frame(&#39;size&#39;) #print(s) plots = s.where(s[&#39;size&#39;] &gt; 1).plot.pie(subplots=True, figsize=(5, 5), legend=False) plots[0].set_title(&#39;Q4: What is the highest level of formal education that you have attained or plan to attain within the next 2 years ?&#39;) print() . . .",
            "url": "https://yannick42.github.io/notebooks/eda/2022/04/22/Kaggle_Survey_2020.html",
            "relUrl": "/eda/2022/04/22/Kaggle_Survey_2020.html",
            "date": " • Apr 22, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://yannick42.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yannick42.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}